{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f412f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import sacrebleu\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "sys.path.append(\"/fs/clip-controllablemt/IWSLT2022/notebooks/\")\n",
    "from mbart_covariate import CMBartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "tgt_lang_to_code = {\n",
    "    \"hi\" : \"hi_IN\",\n",
    "    \"de\" : \"de_DE\",\n",
    "    \"es\" : \"es_XX\",\n",
    "    \"it\" : \"it_IT\",\n",
    "    \"ru\" : \"ru_RU\",\n",
    "    \"ja\" : \"ja_XX\"\n",
    "}\n",
    "\n",
    "direction_to_id = {\n",
    "    \"formal\":1,\n",
    "    \"informal\":2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f2efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fname, n=None):\n",
    "    data = []\n",
    "    i = 0\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            data.append(line.strip())\n",
    "            i+=1\n",
    "            if n is not None and i > n:\n",
    "                break\n",
    "    return data\n",
    "    \n",
    "def get_data( tgt_lang, domain, split, data_dir=\"internal_split\"):\n",
    "    source = read_file(f\"{data_dir}/en-{tgt_lang}/{split}.{domain}.en\")\n",
    "    formal_translations = read_file(f\"{data_dir}/en-{tgt_lang}/{split}.{domain}.formal.{tgt_lang}\")\n",
    "    informal_translations = read_file(f\"{data_dir}/en-{tgt_lang}/{split}.{domain}.informal.{tgt_lang}\")\n",
    "    return source, formal_translations, informal_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb8febd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, tgt_lang, model, tokenizer,  covariate_index=None, strategy=\"greedy\"):\n",
    "    model_inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    kwargs = {}\n",
    "    if covariate_index is not None:\n",
    "        kwargs[\"covariate_ids\"] = torch.tensor([covariate_index]*len(text))\n",
    "    if strategy == \"greedy\":\n",
    "        generated_tokens = model.generate(\n",
    "            **model_inputs,\n",
    "            **kwargs\n",
    "        )\n",
    "    else:\n",
    "        generated_tokens = model.generate(\n",
    "            **model_inputs,\n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang_to_code[tgt_lang]],\n",
    "            max_length=50, \n",
    "            num_beams=5, \n",
    "            num_return_sequences=5, \n",
    "            early_stopping=True,\n",
    "            **kwargs\n",
    "        )\n",
    "    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c114e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir=\"../models/vastai/covariate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3a277b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_direction =\"formal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dacb322",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CMBartForConditionalGeneration.from_pretrained(model_dir, cache_dir=\"/fs/clip-scratch/sweagraw/CACHE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7511a648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CMBartForConditionalGeneration(\n",
       "  (model): CMBartModel(\n",
       "    (covariate): Embedding(3, 1024)\n",
       "    (shared): Embedding(250054, 1024, padding_idx=1)\n",
       "    (encoder): MBartEncoder(\n",
       "      (embed_tokens): Embedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): MBartDecoder(\n",
       "      (embed_tokens): Embedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5b126fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_lang=\"de\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb2eb26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\", src_lang=\"en_XX\", tgt_lang=tgt_lang_to_code[tgt_lang], cache_dir=\"/fs/clip-scratch/sweagraw/CACHE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32cb3d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = read_file(\"../internal_split/en-es/dev.combined.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c70e3ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor([[[-4.7424, -5.0231, -0.5793,  ..., -1.6756, -1.0476, -4.7395]],\n",
      "\n",
      "        [[-4.7424, -5.0231, -0.5793,  ..., -1.6756, -1.0476, -4.7395]],\n",
      "\n",
      "        [[-4.7424, -5.0231, -0.5793,  ..., -1.6756, -1.0476, -4.7395]],\n",
      "\n",
      "        [[-4.7424, -5.0231, -0.5793,  ..., -1.6756, -1.0476, -4.7395]],\n",
      "\n",
      "        [[-4.7424, -5.0231, -0.5793,  ..., -1.6756, -1.0476, -4.7395]]])\n",
      "2 tensor([[[ -5.5482,  -6.6353,  -4.2972,  ..., -10.4037, -13.9227,  -5.8926]],\n",
      "\n",
      "        [[ -1.9137,  -2.6081,   0.8267,  ..., -16.4556,  -4.1660,  -2.2295]],\n",
      "\n",
      "        [[ -2.2839,  -3.3162,   1.8673,  ..., -13.0749,  -7.8504,  -2.5264]],\n",
      "\n",
      "        [[ -3.0832,  -3.4401,   1.8120,  ..., -10.6313,  -7.5578,  -3.1886]],\n",
      "\n",
      "        [[ -4.3922,  -5.5456,  -4.3989,  ..., -11.8608,  -7.7589,  -4.8864]]])\n",
      "3 tensor([[[ -6.0276,  -7.2264,  -8.0568,  ..., -17.3852, -14.8795,  -6.5836]],\n",
      "\n",
      "        [[ -1.2559,  -1.7679,   2.6213,  ...,  -9.9650,  -3.1344,  -1.4084]],\n",
      "\n",
      "        [[ -1.1017,  -1.7379,   3.3292,  ...,  -8.3625,  -5.9727,  -1.3238]],\n",
      "\n",
      "        [[ -8.5509,  -9.3470,  -9.7587,  ..., -18.8819, -13.3617,  -8.6912]],\n",
      "\n",
      "        [[ -7.1955,  -8.1520,  -5.9718,  ..., -23.3250, -12.1263,  -7.6821]]])\n",
      "4 tensor([[[ -1.6907,  -2.4681,  -4.7900,  ...,  -4.2762,  -6.7406,  -2.0221]],\n",
      "\n",
      "        [[ -2.1533,  -2.8248,  -1.5767,  ..., -14.2302,  -6.6064,  -2.3724]],\n",
      "\n",
      "        [[ -2.2624,  -3.2803,  -0.7352,  ..., -13.5903,  -7.8407,  -2.5181]],\n",
      "\n",
      "        [[ -6.1609,  -7.3460,  -9.2962,  ..., -18.4390,  -9.7312,  -6.0256]],\n",
      "\n",
      "        [[ -7.3708,  -8.1536,  -2.9425,  ..., -19.6060, -10.0610,  -7.1398]]])\n",
      "5 tensor([[[ -7.7731,  -9.4797,  -7.3544,  ..., -20.8936, -19.9512,  -8.4790]],\n",
      "\n",
      "        [[ -2.7195,  -3.3552,  -3.8043,  ..., -13.5169,  -4.8179,  -2.9535]],\n",
      "\n",
      "        [[ -4.1620,  -5.0914,  -5.3644,  ..., -16.7539,  -8.6734,  -4.2123]],\n",
      "\n",
      "        [[ -7.9834,  -8.8690,  -2.5015,  ..., -24.9081, -17.0725,  -8.3508]],\n",
      "\n",
      "        [[ -5.0147,  -5.5563, -12.3811,  ..., -21.3123, -12.0546,  -5.0929]]])\n",
      "6 tensor([[[ -6.9362,  -8.4461,  -7.5980,  ..., -17.4553, -17.4699,  -7.3626]],\n",
      "\n",
      "        [[ -7.7125,  -9.0988,  -8.5695,  ..., -18.8450, -23.1781,  -7.8396]],\n",
      "\n",
      "        [[ -4.5355,  -5.2887,  -3.4807,  ..., -14.9023,  -4.3462,  -4.0431]],\n",
      "\n",
      "        [[ -8.9016, -10.1803, -13.7014,  ..., -25.9964, -20.4931,  -9.1641]],\n",
      "\n",
      "        [[ -5.1062,  -5.6891, -13.1911,  ..., -19.3424, -10.2453,  -5.2967]]])\n",
      "7 tensor([[[ -8.6808,  -9.5998,  -8.6355,  ..., -17.3649, -24.8235,  -8.5449]],\n",
      "\n",
      "        [[ -1.1869,  -1.6599,   5.2932,  ...,  -4.1823,  -3.4367,  -1.2581]],\n",
      "\n",
      "        [[ -7.3977,  -8.6430,  -7.8524,  ..., -18.6135, -23.6486,  -7.2954]],\n",
      "\n",
      "        [[ -6.9934,  -8.3228,  -6.9077,  ..., -19.6359, -17.4380,  -7.0769]],\n",
      "\n",
      "        [[ -3.5237,  -4.0768,  -2.0423,  ..., -13.9011,   0.0416,  -3.2349]]])\n",
      "8 tensor([[[-11.1786, -11.8095, -12.5418,  ..., -30.1787, -31.2223, -10.6620]],\n",
      "\n",
      "        [[ -0.4193,  -0.7583,   6.3522,  ...,  -4.0758,  -0.2871,  -0.5047]],\n",
      "\n",
      "        [[ -4.0915,  -5.3644,   1.6334,  ...,  -9.3808, -12.0459,  -4.4314]],\n",
      "\n",
      "        [[ -7.4029,  -8.7474,   1.7676,  ..., -17.9809, -20.4681,  -7.6298]],\n",
      "\n",
      "        [[ -8.7072,  -9.5681,  -9.0231,  ..., -18.5775, -24.9137,  -8.5000]]])\n",
      "9 tensor([[[ -7.6561,  -9.3438,   4.7381,  ..., -30.5423, -29.0452,  -8.0129]],\n",
      "\n",
      "        [[ -0.4731,  -1.2599,   2.5000,  ...,  -7.6266,  -7.7336,  -0.8807]],\n",
      "\n",
      "        [[ -3.8060,  -4.0542,   2.1126,  ...,  -5.7220,  -6.7577,  -3.4564]],\n",
      "\n",
      "        [[ -4.6500,  -5.1648,   3.2708,  ..., -10.0091,  -8.2938,  -4.4476]],\n",
      "\n",
      "        [[ -0.7787,  -1.5849,   2.0159,  ...,  -7.7662,  -8.6108,  -1.2094]]])\n",
      "10 tensor([[[ -3.6658,  -4.5511,   5.0851,  ..., -10.8752, -16.5278,  -3.8085]],\n",
      "\n",
      "        [[ -1.5152,  -2.5077,   0.5411,  ..., -14.8959, -10.1130,  -1.6071]],\n",
      "\n",
      "        [[ -3.0795,  -3.2480,  -5.6880,  ...,  -7.0949,  -6.9937,  -2.8072]],\n",
      "\n",
      "        [[ -1.4811,  -1.7498,  -1.6911,  ...,  -3.6146,  -8.1838,  -1.1107]],\n",
      "\n",
      "        [[ -1.4945,  -2.4871,   0.2848,  ..., -14.0621, -10.8799,  -1.5803]]])\n",
      "11 tensor([[[ -5.6728,  -6.3573,   1.7092,  ..., -19.3781, -21.1771,  -5.3919]],\n",
      "\n",
      "        [[ -0.9059,  -1.4879,   1.4933,  ..., -11.1336,  -8.1271,  -1.1059]],\n",
      "\n",
      "        [[ -0.2940,  -0.7104,   2.7831,  ...,   1.2430,   0.6025,  -0.2663]],\n",
      "\n",
      "        [[ -2.0760,  -2.2544,  -2.1895,  ...,  -1.0894,  -2.4925,  -1.5780]],\n",
      "\n",
      "        [[ -0.9014,  -1.4952,   1.5518,  ..., -10.7925,  -8.2451,  -1.1139]]])\n",
      "12 tensor([[[ -8.9411, -10.7425,  -1.3306,  ..., -29.3527, -25.7433,  -9.6877]],\n",
      "\n",
      "        [[ -2.1192,  -3.2028,   1.9571,  ..., -18.3442,  -6.3671,  -1.8294]],\n",
      "\n",
      "        [[ -1.0586,  -1.4333,   1.6585,  ...,  -4.2075,  -2.6408,  -1.1986]],\n",
      "\n",
      "        [[ -0.4742,  -1.0176,   1.3551,  ...,  -2.9947,  -1.2729,  -0.6785]],\n",
      "\n",
      "        [[ -2.2494,  -3.3411,   1.7209,  ..., -18.6708,  -7.1044,  -1.9520]]])\n",
      "13 tensor([[[ -6.2066,  -7.2006,   5.6518,  ..., -22.6558, -21.8810,  -6.1488]],\n",
      "\n",
      "        [[ -1.4482,  -1.7861,   1.7603,  ...,  -7.6464,  -5.2858,  -1.5068]],\n",
      "\n",
      "        [[ -1.7061,  -1.9981,   1.1618,  ...,  -6.0972,  -5.8760,  -1.6494]],\n",
      "\n",
      "        [[ -3.2341,  -3.4076,   1.0513,  ...,  -8.1667,  -7.5975,  -2.7676]],\n",
      "\n",
      "        [[ -3.2538,  -3.6437,   0.5442,  ..., -10.5939,  -9.4778,  -2.5056]]])\n",
      "14 tensor([[[ -1.6436,  -2.6525,  -0.4197,  ..., -12.8829,  -2.3410,  -1.7980]],\n",
      "\n",
      "        [[ -1.7512,  -2.7666,  -0.5891,  ..., -12.5124,  -2.7040,  -1.8838]],\n",
      "\n",
      "        [[ -5.1811,  -5.1193,   3.6451,  ..., -16.1021, -14.9279,  -4.4040]],\n",
      "\n",
      "        [[ -3.6767,  -4.7474,  -0.3674,  ...,  -7.7738,  -8.4080,  -3.0159]],\n",
      "\n",
      "        [[ -8.1728,  -8.4217,  -6.3734,  ..., -23.0189, -23.4961,  -7.8390]]])\n",
      "15 tensor([[[ -7.5074,  -7.8000,  -4.9002,  ..., -28.1144, -21.3268,  -7.1322]],\n",
      "\n",
      "        [[ -1.9028,  -3.0623,   0.7313,  ..., -15.1236,  -2.3178,  -2.2005]],\n",
      "\n",
      "        [[ -5.7539,  -6.3694,  -6.9319,  ..., -14.7286, -11.6900,  -5.6743]],\n",
      "\n",
      "        [[ -7.1623,  -8.0289,  -8.9148,  ..., -22.0422, -25.3034,  -7.6404]],\n",
      "\n",
      "        [[ -1.9980,  -3.1890,   0.8024,  ..., -14.8504,  -2.4895,  -2.3178]]])\n",
      "16 tensor([[[ -6.9934,  -7.9201,  -8.5104,  ..., -22.1595, -24.9731,  -7.4929]],\n",
      "\n",
      "        [[ -5.7267,  -6.3121,  -6.8871,  ..., -16.1678, -12.0255,  -5.6905]],\n",
      "\n",
      "        [[ -9.4292,  -9.5727,  -2.0434,  ..., -20.0836, -21.5201,  -8.7647]],\n",
      "\n",
      "        [[ -3.7751,  -4.6130,   0.2290,  ..., -19.7988,  -9.9301,  -4.1381]],\n",
      "\n",
      "        [[ -5.7399,  -6.3208,  -7.0294,  ..., -16.3081, -12.2898,  -5.7011]]])\n",
      "17 tensor([[[ -9.5131,  -9.6355,  -2.8239,  ..., -20.1302, -20.3963,  -8.8467]],\n",
      "\n",
      "        [[ -3.5161,  -4.3412,   1.2844,  ..., -19.3967,  -9.0617,  -3.9305]],\n",
      "\n",
      "        [[ -9.3238,  -9.4179, -11.8206,  ..., -23.8636, -23.9819,  -8.7410]],\n",
      "\n",
      "        [[ -3.5550,  -4.3694,   1.6325,  ..., -19.3414,  -9.0836,  -3.9672]],\n",
      "\n",
      "        [[ -0.9082,  -1.4347,   0.9833,  ..., -15.1208,  -5.0526,  -0.8398]]])\n",
      "18 tensor([[[ -9.4679,  -9.5363, -11.9515,  ..., -24.0210, -24.2381,  -8.8626]],\n",
      "\n",
      "        [[-11.3740, -11.7474,  -9.1449,  ..., -23.6955, -22.7851, -11.3075]],\n",
      "\n",
      "        [[ -0.8826,  -1.3499,   1.4141,  ..., -15.0399,  -4.3391,  -0.7830]],\n",
      "\n",
      "        [[ -0.8863,  -1.3620,   1.4906,  ..., -15.0552,  -4.3117,  -0.7929]],\n",
      "\n",
      "        [[ -1.2579,  -1.8931,   1.7929,  ..., -10.2118,  -2.6251,  -1.5898]]])\n",
      "19 tensor([[[-11.4447, -11.7980,  -9.1379,  ..., -23.5322, -22.7482, -11.3595]],\n",
      "\n",
      "        [[ -6.7266,  -6.4538,  -1.2984,  ..., -21.2057, -22.7540,  -5.8679]],\n",
      "\n",
      "        [[ -1.3113,  -1.9334,   1.9181,  ..., -10.4755,  -2.6706,  -1.6393]],\n",
      "\n",
      "        [[ -0.8744,  -1.2579,   6.0712,  ...,  -8.5436,  -7.4476,  -0.7403]],\n",
      "\n",
      "        [[ -1.3156,  -1.9477,   2.0503,  ..., -10.4994,  -2.7538,  -1.6502]]])\n",
      "20 tensor([[[ -6.8122,  -6.5590,  -1.4578,  ..., -21.6721, -22.8295,  -5.9749]],\n",
      "\n",
      "        [[ -6.5803,  -6.9032,   1.3975,  ..., -15.8783, -21.3633,  -5.9057]],\n",
      "\n",
      "        [[ -0.7747,  -1.0788,   6.4029,  ...,  -8.7815,  -7.4671,  -0.5991]],\n",
      "\n",
      "        [[ -0.7601,  -1.0699,   6.2477,  ...,  -8.6025,  -7.3198,  -0.6075]],\n",
      "\n",
      "        [[  0.2709,  -0.2611,   5.8102,  ...,  -1.7184,   2.9317,  -0.0446]]])\n",
      "21 tensor([[[-6.6317e+00, -6.9615e+00,  1.2501e+00,  ..., -1.5931e+01,\n",
      "          -2.1180e+01, -5.9681e+00]],\n",
      "\n",
      "        [[ 3.0741e-01, -2.0601e-01,  5.8746e+00,  ..., -1.7425e+00,\n",
      "           2.8633e+00,  7.6489e-03]],\n",
      "\n",
      "        [[-1.9364e+00, -2.5076e+00,  5.5606e+00,  ..., -6.3384e+00,\n",
      "          -2.0749e+00, -2.1925e+00]],\n",
      "\n",
      "        [[ 2.6528e-01, -2.5229e-01,  5.8025e+00,  ..., -1.8029e+00,\n",
      "           2.8143e+00, -4.1937e-02]],\n",
      "\n",
      "        [[-5.6527e+00, -6.0767e+00,  6.5185e+00,  ..., -2.3101e+01,\n",
      "          -1.5458e+01, -5.4720e+00]]])\n",
      "22 tensor([[[ -1.7166,  -2.3191,   5.8747,  ...,  -7.1482,  -2.5356,  -1.9805]],\n",
      "\n",
      "        [[ -1.5938,  -2.1938,   5.7761,  ...,  -6.7017,  -2.2589,  -1.8650]],\n",
      "\n",
      "        [[ -5.5640,  -5.9991,   6.7461,  ..., -23.0463, -15.3698,  -5.4006]],\n",
      "\n",
      "        [[ -1.7683,  -2.4938,   7.6183,  ...,  -3.9116,  -0.6486,  -2.3048]],\n",
      "\n",
      "        [[ -0.0430,  -0.1709,  17.7135,  ...,  -0.1138,   0.1087,  -0.0687]]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 tensor([[[ -1.6418,  -2.4584,   7.9762,  ...,  -6.0222,  -2.7805,  -2.2473]],\n",
      "\n",
      "        [[ -0.0429,  -0.1661,  17.7253,  ...,  -0.0416,   0.1301,  -0.0636]],\n",
      "\n",
      "        [[ -1.5832,  -2.4155,   7.6438,  ...,  -5.6599,  -2.6128,  -2.1941]],\n",
      "\n",
      "        [[ -3.0845,  -3.7446,   4.0761,  ..., -15.1694,  -9.4091,  -3.1554]],\n",
      "\n",
      "        [[ -2.3498,  -3.1403,   2.5075,  ..., -14.4866,  -9.7566,  -2.5350]]])\n",
      "24 tensor([[[ -3.0105,  -3.6368,   4.1565,  ..., -15.3424, -10.0046,  -3.0647]],\n",
      "\n",
      "        [[ -1.5074,  -1.8657,   7.7108,  ..., -10.8266,  -5.3680,  -1.5737]],\n",
      "\n",
      "        [[ -2.9646,  -3.6254,   4.3295,  ..., -15.2370,  -9.8587,  -3.0174]],\n",
      "\n",
      "        [[ -2.2658,  -3.0358,   2.8230,  ..., -14.6023, -10.1063,  -2.4420]],\n",
      "\n",
      "        [[ -0.9081,  -1.2261,   8.2119,  ...,  -8.3735,  -4.1668,  -0.9815]]])\n",
      "25 tensor([[[ -1.5382,  -1.8889,   8.0689,  ..., -10.8068,  -5.4506,  -1.5931]],\n",
      "\n",
      "        [[ -0.0327,  -0.2110,  17.9628,  ...,  -1.2807,   1.4624,  -0.1546]],\n",
      "\n",
      "        [[ -1.6910,  -2.0405,   7.4916,  ..., -11.3284,  -5.9514,  -1.7642]],\n",
      "\n",
      "        [[ -0.9655,  -1.2821,   8.4491,  ...,  -8.4586,  -4.2354,  -1.0346]],\n",
      "\n",
      "        [[ -0.0520,  -0.2326,  17.9628,  ...,  -1.2787,   1.4700,  -0.1725]]])\n",
      "26 tensor([[[-2.3314e-02, -1.9751e-01,  1.7976e+01,  ..., -1.2114e+00,\n",
      "           1.4632e+00, -1.4110e-01]],\n",
      "\n",
      "        [[ 7.1056e-03, -1.5257e-01,  1.7969e+01,  ..., -1.1846e+00,\n",
      "           1.4912e+00, -9.8363e-02]],\n",
      "\n",
      "        [[-3.9245e-02, -2.1764e-01,  1.7976e+01,  ..., -1.2111e+00,\n",
      "           1.4576e+00, -1.5716e-01]],\n",
      "\n",
      "        [[-5.5638e-01, -8.3846e-01,  8.1641e+00,  ..., -8.3270e+00,\n",
      "          -3.6552e+00, -7.1856e-01]],\n",
      "\n",
      "        [[-9.4768e-01, -1.2328e+00,  7.6077e+00,  ..., -1.0249e+01,\n",
      "          -4.6582e+00, -1.1392e+00]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Jo, to dává smysl, slyšel jste o tom bunkru za 10 milionů dolarů, co má?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_text([source[4]], tgt_lang, model, tokenizer, 1)[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74ec60ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor([[[-4.7464, -5.0151, -0.5664,  ..., -1.6514, -1.1374, -4.7402]],\n",
      "\n",
      "        [[-4.7464, -5.0151, -0.5664,  ..., -1.6514, -1.1374, -4.7402]],\n",
      "\n",
      "        [[-4.7464, -5.0151, -0.5664,  ..., -1.6514, -1.1374, -4.7402]],\n",
      "\n",
      "        [[-4.7464, -5.0151, -0.5664,  ..., -1.6514, -1.1374, -4.7402]],\n",
      "\n",
      "        [[-4.7464, -5.0151, -0.5664,  ..., -1.6514, -1.1374, -4.7402]]])\n",
      "2 tensor([[[ -5.6712,  -6.7244,  -4.5681,  ..., -10.6964, -15.3725,  -5.9849]],\n",
      "\n",
      "        [[ -1.7648,  -2.4561,   1.1433,  ..., -16.2529,  -4.8401,  -2.0909]],\n",
      "\n",
      "        [[ -4.5754,  -5.7117,  -4.6680,  ..., -12.9725,  -9.6723,  -5.0037]],\n",
      "\n",
      "        [[ -2.1776,  -3.3012,   2.3210,  ..., -13.7446,  -8.4125,  -2.4810]],\n",
      "\n",
      "        [[ -3.1118,  -4.2364,   2.9570,  ...,  -6.4048, -10.9913,  -3.1200]]])\n",
      "3 tensor([[[ -6.0094,  -7.2505,  -8.2941,  ..., -18.0351, -16.3461,  -6.5825]],\n",
      "\n",
      "        [[ -1.0057,  -1.5278,   2.9936,  ...,  -9.5703,  -3.3526,  -1.1647]],\n",
      "\n",
      "        [[ -0.9703,  -1.6195,   3.5106,  ...,  -8.4079,  -6.2536,  -1.2160]],\n",
      "\n",
      "        [[ -1.3876,  -2.2172,   4.3457,  ...,  -3.5988,  -8.5488,  -1.6745]],\n",
      "\n",
      "        [[ -7.1137,  -8.0656,  -5.7078,  ..., -23.6203, -14.0153,  -7.6239]]])\n",
      "4 tensor([[[ -1.8104,  -2.6113,  -4.5300,  ...,  -3.9813,  -7.7262,  -2.1611]],\n",
      "\n",
      "        [[ -1.8712,  -2.6013,  -1.3674,  ..., -14.0880,  -7.4889,  -2.1140]],\n",
      "\n",
      "        [[ -2.0994,  -3.1447,  -0.5034,  ..., -13.3472,  -8.0462,  -2.3829]],\n",
      "\n",
      "        [[ -1.9898,  -3.1789,   0.6949,  ...,  -6.2693, -11.5390,  -2.4816]],\n",
      "\n",
      "        [[ -6.0436,  -7.2548,  -9.5703,  ..., -19.5170, -11.1425,  -5.9841]]])\n",
      "5 tensor([[[ -7.7214,  -9.4371,  -6.6116,  ..., -21.2738, -21.0159,  -8.3672]],\n",
      "\n",
      "        [[ -2.5485,  -3.2115,  -3.4962,  ..., -13.5862,  -5.5641,  -2.8259]],\n",
      "\n",
      "        [[ -3.8588,  -4.8828,  -4.8060,  ..., -16.7549,  -9.2186,  -4.0151]],\n",
      "\n",
      "        [[ -3.8733,  -5.1325,  -1.0096,  ..., -17.6037, -13.5745,  -4.1388]],\n",
      "\n",
      "        [[ -2.7425,  -4.0730,  -1.1954,  ...,  -8.6199, -11.5326,  -3.3689]]])\n",
      "6 tensor([[[ -7.6736,  -9.0161,  -8.8614,  ..., -18.9997, -24.4483,  -7.8526]],\n",
      "\n",
      "        [[ -6.8043,  -8.3800,  -6.9979,  ..., -17.3267, -18.2906,  -7.2248]],\n",
      "\n",
      "        [[ -8.8729, -10.2051, -13.6460,  ..., -26.3322, -21.9634,  -9.1190]],\n",
      "\n",
      "        [[ -4.7221,  -5.4797,  -3.3865,  ..., -15.8569,  -5.9885,  -4.2852]],\n",
      "\n",
      "        [[ -4.9588,  -5.5675, -12.7976,  ..., -19.4112, -11.3893,  -5.1550]]])\n",
      "7 tensor([[[ -6.7982,  -8.2102,  -6.1151,  ..., -19.1496, -18.2032,  -6.9202]],\n",
      "\n",
      "        [[ -7.4328,  -8.6231,  -8.1308,  ..., -18.7194, -24.8651,  -7.3603]],\n",
      "\n",
      "        [[ -1.1654,  -1.6036,   5.7300,  ...,  -4.0598,  -3.7485,  -1.2404]],\n",
      "\n",
      "        [[ -3.4387,  -3.9731,  -1.6774,  ..., -14.2880,  -1.2406,  -3.1932]],\n",
      "\n",
      "        [[ -8.4395, -10.2811,  -8.5309,  ..., -20.5214, -24.9355,  -8.6488]]])\n",
      "8 tensor([[[ -0.3111,  -0.6338,   6.7106,  ...,  -3.8583,  -0.6883,  -0.3869]],\n",
      "\n",
      "        [[ -3.9194,  -5.1162,   2.5109,  ..., -10.1469, -12.6030,  -4.2283]],\n",
      "\n",
      "        [[ -7.4218,  -8.7828,   1.9405,  ..., -20.2506, -21.6666,  -7.6917]],\n",
      "\n",
      "        [[ -7.2220,  -8.4071,  -7.8672,  ..., -18.9406, -24.8661,  -7.0894]],\n",
      "\n",
      "        [[ -7.5426,  -9.5403,  -6.3269,  ..., -18.1536, -23.3436,  -7.6671]]])\n",
      "9 tensor([[[ -3.5590,  -3.7830,   2.3229,  ...,  -5.1691,  -8.3517,  -3.2299]],\n",
      "\n",
      "        [[ -4.5467,  -5.0483,   3.4398,  ...,  -9.8515,  -9.3546,  -4.3783]],\n",
      "\n",
      "        [[ -1.1952,  -1.9615,   1.4987,  ...,  -9.9464,  -6.0228,  -1.3641]],\n",
      "\n",
      "        [[ -0.9002,  -1.6543,   1.8932,  ..., -10.7697,  -7.2293,  -1.0302]],\n",
      "\n",
      "        [[ -0.4829,  -1.1837,   2.8107,  ...,  -7.3626,  -7.6898,  -0.7855]]])\n",
      "10 tensor([[[ -3.0187,  -3.1715,  -5.9180,  ...,  -7.5724,  -8.3344,  -2.7538]],\n",
      "\n",
      "        [[ -1.2839,  -1.5439,  -1.3696,  ...,  -3.8082,  -9.2734,  -0.8994]],\n",
      "\n",
      "        [[ -1.1886,  -2.1385,   1.5540,  ..., -15.4808, -12.4592,  -1.3947]],\n",
      "\n",
      "        [[ -1.3633,  -2.3476,   1.5742,  ..., -17.0170, -12.5443,  -1.4997]],\n",
      "\n",
      "        [[ -1.5146,  -2.5257,   0.7073,  ..., -15.7150, -11.2430,  -1.6266]]])\n",
      "11 tensor([[[-1.5237e-01, -5.3224e-01,  3.1059e+00,  ...,  1.5931e+00,\n",
      "          -3.1912e-04, -1.1241e-01]],\n",
      "\n",
      "        [[-1.8500e+00, -1.9027e+00, -2.0385e+00,  ..., -7.7220e-01,\n",
      "          -3.3203e+00, -1.2251e+00]],\n",
      "\n",
      "        [[-7.7452e-01, -1.3790e+00,  1.7931e+00,  ..., -1.0692e+01,\n",
      "          -8.8957e+00, -9.9475e-01]],\n",
      "\n",
      "        [[-7.5745e-01, -1.3484e+00,  1.5813e+00,  ..., -1.0744e+01,\n",
      "          -8.8258e+00, -9.6256e-01]],\n",
      "\n",
      "        [[-8.4451e-01, -1.4327e+00,  1.7161e+00,  ..., -1.1395e+01,\n",
      "          -8.9421e+00, -1.0570e+00]]])\n",
      "12 tensor([[[ -0.4400,  -0.9909,   1.7390,  ...,  -2.3067,  -1.5456,  -0.6104]],\n",
      "\n",
      "        [[ -0.8502,  -1.2836,   2.1018,  ...,  -3.2189,  -2.4928,  -0.9579]],\n",
      "\n",
      "        [[ -2.3046,  -3.3403,   2.1539,  ..., -18.3420,  -7.7218,  -2.0040]],\n",
      "\n",
      "        [[ -2.1533,  -3.1797,   2.2290,  ..., -17.9504,  -6.9811,  -1.8518]],\n",
      "\n",
      "        [[ -2.0639,  -3.1034,   2.4811,  ..., -17.4930,  -6.7625,  -1.7671]]])\n",
      "13 tensor([[[-1.2255, -1.5916,  1.8015,  ..., -9.1649, -7.0487, -1.2730]],\n",
      "\n",
      "        [[-3.1349, -3.2936,  1.3260,  ..., -7.8134, -7.9940, -2.6509]],\n",
      "\n",
      "        [[-1.5371, -1.8782,  1.2242,  ..., -8.3534, -7.6128, -1.4928]],\n",
      "\n",
      "        [[-3.0877, -3.2587,  1.2870,  ..., -7.8701, -7.9235, -2.6233]],\n",
      "\n",
      "        [[-0.2903, -0.8173,  1.4650,  ..., -2.8890, -0.7433, -0.4278]]])\n",
      "14 tensor([[[ -1.5478,  -2.5221,  -0.2746,  ..., -12.4514,  -3.3952,  -1.6665]],\n",
      "\n",
      "        [[ -1.6786,  -2.6480,  -0.4197,  ..., -12.2632,  -3.8151,  -1.7804]],\n",
      "\n",
      "        [[ -3.2193,  -4.2934,   0.2807,  ...,  -6.1367,  -7.4066,  -2.5140]],\n",
      "\n",
      "        [[ -1.6452,  -1.9570,   1.0354,  ...,  -8.3287,  -7.6048,  -1.6249]],\n",
      "\n",
      "        [[ -3.4256,  -4.5530,  -0.0709,  ...,  -6.7996,  -8.1875,  -2.7804]]])\n",
      "15 tensor([[[ -1.8745,  -2.9663,   1.1002,  ..., -14.8005,  -3.4527,  -2.1414]],\n",
      "\n",
      "        [[ -5.5921,  -6.1904,  -6.5543,  ..., -14.2947, -12.1702,  -5.4741]],\n",
      "\n",
      "        [[ -1.6427,  -2.6114,  -0.3864,  ..., -12.0788,  -3.7499,  -1.7455]],\n",
      "\n",
      "        [[ -1.9933,  -3.1082,   1.1471,  ..., -14.6771,  -3.6923,  -2.2740]],\n",
      "\n",
      "        [[ -3.1867,  -3.5402,  -0.4818,  ..., -15.0497,  -8.0245,  -2.9045]]])\n",
      "16 tensor([[[ -5.5331,  -6.1192,  -6.4848,  ..., -15.8295, -12.5976,  -5.4766]],\n",
      "\n",
      "        [[ -3.6391,  -4.4672,   0.8052,  ..., -19.2990, -10.6964,  -3.9914]],\n",
      "\n",
      "        [[ -5.5236,  -6.1124,  -6.5547,  ..., -15.9228, -12.8200,  -5.4637]],\n",
      "\n",
      "        [[ -1.9655,  -3.0792,   1.1441,  ..., -14.6154,  -3.6759,  -2.2470]],\n",
      "\n",
      "        [[ -2.4520,  -3.5962,   1.1296,  ..., -11.3151, -12.8498,  -2.4905]]])\n",
      "17 tensor([[[ -3.2794,  -4.0777,   1.9361,  ..., -18.6325,  -9.5967,  -3.6757]],\n",
      "\n",
      "        [[ -0.9219,  -1.4178,   1.2712,  ..., -14.3329,  -6.1309,  -0.8331]],\n",
      "\n",
      "        [[ -3.3143,  -4.1060,   2.2796,  ..., -18.6392,  -9.7184,  -3.7104]],\n",
      "\n",
      "        [[ -5.4982,  -6.0883,  -6.4860,  ..., -15.8808, -12.7714,  -5.4376]],\n",
      "\n",
      "        [[ -2.8643,  -3.5577,   5.4406,  ...,  -7.7893,  -8.4154,  -2.9573]]])\n",
      "18 tensor([[[ -0.8920,  -1.3533,   1.6727,  ..., -14.3805,  -5.5134,  -0.7809]],\n",
      "\n",
      "        [[ -1.2325,  -1.8252,   2.1634,  ..., -10.2914,  -3.5995,  -1.5023]],\n",
      "\n",
      "        [[ -0.8784,  -1.3453,   1.7689,  ..., -14.4086,  -5.5249,  -0.7676]],\n",
      "\n",
      "        [[ -3.3048,  -4.0919,   2.0384,  ..., -18.6378,  -9.7041,  -3.6981]],\n",
      "\n",
      "        [[ -3.2445,  -4.2098,   6.4199,  ...,  -7.6655,  -9.1768,  -3.6968]]])\n",
      "19 tensor([[[ -1.2828,  -1.8655,   2.3025,  ..., -10.5712,  -3.6683,  -1.5504]],\n",
      "\n",
      "        [[ -0.8404,  -1.2156,   6.8459,  ...,  -8.4084,  -8.8285,  -0.7226]],\n",
      "\n",
      "        [[ -1.2905,  -1.8820,   2.4377,  ..., -10.6513,  -3.7678,  -1.5596]],\n",
      "\n",
      "        [[ -0.8755,  -1.3403,   1.7115,  ..., -14.4538,  -5.5502,  -0.7674]],\n",
      "\n",
      "        [[ -1.2000,  -1.7536,   4.6929,  ..., -11.1746,  -7.6741,  -1.1051]]])\n",
      "20 tensor([[[ -0.7776,  -1.0932,   7.0141,  ...,  -8.8028,  -8.9743,  -0.6337]],\n",
      "\n",
      "        [[  0.2508,  -0.2729,   6.2000,  ...,  -1.8128,   2.4577,  -0.0638]],\n",
      "\n",
      "        [[ -0.7648,  -1.0867,   6.7547,  ...,  -8.7814,  -8.9861,  -0.6439]],\n",
      "\n",
      "        [[ -1.2708,  -1.8570,   2.4120,  ..., -10.6164,  -3.7617,  -1.5408]],\n",
      "\n",
      "        [[ -4.0699,  -5.2469,   0.3015,  ..., -21.8664, -18.9335,  -4.5121]]])\n",
      "21 tensor([[[ 2.9596e-01, -2.0895e-01,  6.2629e+00,  ..., -1.7799e+00,\n",
      "           2.4016e+00, -4.3949e-03]],\n",
      "\n",
      "        [[-1.7570e+00, -2.3612e+00,  6.0399e+00,  ..., -6.0456e+00,\n",
      "          -2.7135e+00, -2.0444e+00]],\n",
      "\n",
      "        [[-7.7139e-01, -1.0788e+00,  6.3038e+00,  ..., -8.7008e+00,\n",
      "          -8.9664e+00, -6.4450e-01]],\n",
      "\n",
      "        [[ 2.4633e-01, -2.6723e-01,  6.2137e+00,  ..., -1.8650e+00,\n",
      "           2.3789e+00, -6.7066e-02]],\n",
      "\n",
      "        [[-2.6387e-01, -6.7752e-01,  9.7969e+00,  ..., -5.0699e+00,\n",
      "          -2.9153e+00, -3.9184e-01]]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 tensor([[[-1.5584e+00, -2.1660e+00,  6.3692e+00,  ..., -6.7219e+00,\n",
      "          -2.9220e+00, -1.8416e+00]],\n",
      "\n",
      "        [[-1.4917e+00, -2.2727e+00,  8.8907e+00,  ..., -3.5433e+00,\n",
      "          -1.9351e+00, -2.0552e+00]],\n",
      "\n",
      "        [[-1.4591e+00, -2.0585e+00,  6.1580e+00,  ..., -6.4600e+00,\n",
      "          -2.7980e+00, -1.7495e+00]],\n",
      "\n",
      "        [[ 4.5345e-02, -1.2175e-01,  1.8114e+01,  ...,  4.2065e-02,\n",
      "           2.0281e-01, -7.5166e-03]],\n",
      "\n",
      "        [[ 2.4338e-01, -2.6677e-01,  6.1952e+00,  ..., -1.8593e+00,\n",
      "           2.3992e+00, -6.6405e-02]]])\n",
      "23 tensor([[[-1.3857e+00, -2.2391e+00,  9.2652e+00,  ..., -5.5842e+00,\n",
      "          -3.5953e+00, -2.0152e+00]],\n",
      "\n",
      "        [[-2.8584e+00, -3.4720e+00,  4.1527e+00,  ..., -1.4321e+01,\n",
      "          -1.0640e+01, -2.9010e+00]],\n",
      "\n",
      "        [[-1.5109e+00, -2.0991e+00,  5.6305e+00,  ..., -6.5982e+00,\n",
      "          -2.9278e+00, -1.7922e+00]],\n",
      "\n",
      "        [[-1.3797e+00, -2.2439e+00,  8.9240e+00,  ..., -5.2480e+00,\n",
      "          -3.6052e+00, -2.0049e+00]],\n",
      "\n",
      "        [[ 1.2032e-02, -2.1490e-01,  1.8066e+01,  ..., -2.0103e+00,\n",
      "           6.6297e-01, -1.6665e-01]]])\n",
      "24 tensor([[[ -2.7854,  -3.3775,   4.2455,  ..., -14.3696, -11.0932,  -2.8286]],\n",
      "\n",
      "        [[ -1.5717,  -1.9254,   7.7835,  ..., -11.4104,  -6.8615,  -1.6623]],\n",
      "\n",
      "        [[ -2.7796,  -3.3986,   4.4196,  ..., -14.4476, -11.0505,  -2.8096]],\n",
      "\n",
      "        [[ -1.3401,  -2.2068,   8.7999,  ...,  -5.1254,  -3.4718,  -1.9724]],\n",
      "\n",
      "        [[ -2.0324,  -2.7446,   2.8528,  ..., -13.5803, -11.0338,  -2.1857]]])\n",
      "25 tensor([[[ -1.5621,  -1.9104,   8.2295,  ..., -11.1288,  -6.7692,  -1.6509]],\n",
      "\n",
      "        [[  0.0258,  -0.1541,  18.0280,  ...,  -1.3780,   0.9598,  -0.1177]],\n",
      "\n",
      "        [[ -1.6889,  -2.0313,   7.6571,  ..., -11.7509,  -7.2348,  -1.7926]],\n",
      "\n",
      "        [[ -2.7671,  -3.3790,   4.4249,  ..., -14.4970, -11.0691,  -2.7934]],\n",
      "\n",
      "        [[ -1.0488,  -1.3753,   8.5206,  ...,  -8.5754,  -5.4430,  -1.1475]]])\n",
      "26 tensor([[[  0.0357,  -0.1393,  18.0406,  ...,  -1.3119,   0.9666,  -0.1030]],\n",
      "\n",
      "        [[  0.0514,  -0.1067,  18.0207,  ...,  -1.3179,   0.9574,  -0.0712]],\n",
      "\n",
      "        [[ -1.6445,  -1.9831,   7.4479,  ..., -11.7167,  -7.1841,  -1.7463]],\n",
      "\n",
      "        [[  0.0270,  -0.1543,  18.0503,  ...,  -1.3242,   0.9583,  -0.1121]],\n",
      "\n",
      "        [[ -0.5120,  -0.7890,   8.5245,  ...,  -8.2703,  -4.7602,  -0.6839]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Jo, to dává smysl, slyšel jsi o tom bunkru za 10 milionů dolarů, co má?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_text([source[4]], tgt_lang, model, tokenizer, 2)[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c5cb83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9adde4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
